# -*- coding: UTF-8 -*-
'''
================================================
*      CREATE ON: 2024/07/23 15:28:23
*      AUTHOR: @Junyin Xiong
*      DESCRIPTION: è®­ç»ƒæµç¨‹
=================================================
'''

import os
import time
import shutil
import torch
import pandas as pd
import logging
import readline # è§£å†³input()æ— æ³•ä½¿ç”¨Backspaceçš„é—®é¢˜, âš ï¸ä¸èƒ½åˆ æ‰

from tabulate import tabulate
from torch.utils.tensorboard import SummaryWriter 

from evaluate.inference import inference
from train_and_val import train_one_epoch, val_one_epoch

from utils.ckpt_tools import *
from utils.logger_tools import *
from utils.shell_tools import *

from torchinfo import summary


# constant
TB_PORT = 6007
RANDOM_SEED = 42
scheduler_start_epoch = 0
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

os.environ["CUDA_LAUNCH_BLOCKING"] = '1'
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = True

torch.manual_seed(RANDOM_SEED)
torch.cuda.manual_seed(RANDOM_SEED)                 #è®©æ˜¾å¡äº§ç”Ÿçš„éšæœºæ•°ä¸€è‡´

logger = logging.getLogger(__name__)

# å®šä¹‰æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def train(model, Metrics, train_loader,  val_loader, test_loader, scaler, optimizer, scheduler, loss_function, 
          num_epochs, device, results_dir, logs_path, output_path, start_epoch, best_val_loss, test_csv,
          tb=False,  
          interval=10, 
          save_max=10, 
          early_stopping_patience=10,
          resume_tb_path=None,
          ):
    """
    æ¨¡å‹è®­ç»ƒæµç¨‹
    :param model: æ¨¡å‹
    :param train_loader: è®­ç»ƒæ•°æ®é›†
    :param val_loader: éªŒè¯æ•°æ®é›†
    :param optimizer: ä¼˜åŒ–å™¨
    :param loss_function: åˆç§° criterion æŸå¤±å‡½æ•°
    :param num_epochs: è®­ç»ƒè½®æ•°
    :param device: è®¾å¤‡
    """
    best_epoch = 0
    save_counter = 0
    early_stopping_counter = 0
    date_time_str = get_current_date() + '_' + get_current_time()
    end_epoch = start_epoch + num_epochs
    model_name = model.__class__.__name__
    optimizer_name = optimizer.__class__.__name__
    scheduler_name = scheduler.__class__.__name__
    loss_func_name = loss_function.__class__.__name__
    test_df = pd.read_csv(test_csv)

    if resume_tb_path:
        tb_dir = resume_tb_path
    else:
        tb_dir = os.path.join(results_dir, f'tensorBoard')
    ckpt_dir = os.path.join(results_dir, f'checkpoints')
    os.makedirs(ckpt_dir, exist_ok=True)

    if scheduler:
        current_lr = scheduler.get_last_lr()[0]
    else:
        current_lr = optimizer.param_groups[0]["lr"]
    # åˆå§‹åŒ–æ—¥å¿—
    logger.info(f'å¼€å§‹è®­ç»ƒ, è®­ç»ƒè½®æ•°:{num_epochs}, {model_name}æ¨¡å‹å†™å…¥tensorBoard, ä½¿ç”¨ {optimizer_name} ä¼˜åŒ–å™¨, å­¦ä¹ ç‡: {current_lr}, æŸå¤±å‡½æ•°: {loss_func_name}')

    writer = SummaryWriter(tb_dir)
    
    # æ·»åŠ æ¨¡å‹ç»“æ„åˆ°tensorboard
    writer.add_graph(model, input_to_model=torch.rand(1, 4, 128, 128, 128).to(DEVICE))
    print(f'{model_name}æ¨¡å‹å†™å…¥tensorBoard, ä½¿ç”¨ {optimizer_name} ä¼˜åŒ–å™¨, å­¦ä¹ ç‡: {optimizer.param_groups[0]["lr"]}, æŸå¤±å‡½æ•°: {loss_func_name}')
    
    for epoch in range(start_epoch, end_epoch):
        epoch += 1
        if scheduler:
            current_lr = scheduler.get_last_lr()[0]
        else:
            current_lr = optimizer.param_groups[0]["lr"]
        """-------------------------------------- è®­ç»ƒè¿‡ç¨‹ --------------------------------------------------"""
        print(f"=== Training on [Epoch {epoch}/{end_epoch}] ===:")
        
        train_mean_loss = 0.0
        start_time = time.time()
        # è®­ç»ƒ
        train_running_loss, train_et_loss, train_tc_loss, train_wt_loss = train_one_epoch(model, train_loader, scaler, optimizer, loss_function, device)
        
        # è®¡ç®—å¹³å‡loss
        train_mean_loss =  train_running_loss / len(train_loader) 
        mean_train_et_loss = train_et_loss / len(train_loader)
        mean_train_tc_loss = train_tc_loss / len(train_loader)
        mean_train_wt_loss = train_wt_loss / len(train_loader)
        
        if scheduler_name == 'CosineAnnealingLR' and epoch > scheduler_start_epoch: # ä»ç¬¬20ä¸ªepochå¼€å§‹ï¼Œä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡
            scheduler.step()                    # æ¯ç§è°ƒåº¦å™¨çš„stepæ–¹æ³•ä¸åŒï¼Œä¼ å…¥çš„å‚æ•°ä¹Ÿä¸ä¸€æ ·
        writer.add_scalars(f'{loss_func_name}/train',
                           {'Mean':train_mean_loss, 'ET': mean_train_et_loss, 'TC': mean_train_tc_loss, 'WT': mean_train_wt_loss}, epoch)
        end_time = time.time()
        train_cost_time = end_time - start_time
        # print info
        print(f"- Train mean loss: {train_mean_loss:.4f}\n"
              f"- ET loss: {mean_train_et_loss:.4f}\n"
              f"- TC loss: {mean_train_tc_loss:.4f}\n"
              f"- WT loss: {mean_train_wt_loss:.4f}\n"
              f"- Cost time: {train_cost_time/60:.2f}mins â±ï¸\n")
        
        """-------------------------------------- éªŒè¯è¿‡ç¨‹ --------------------------------------------------"""
        if (epoch) % interval == 0:
            print(f"=== Validating on [Epoch {epoch}/{end_epoch}] ===:")
            
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()
            
            # éªŒè¯
            val_running_loss, val_et_loss, val_tc_loss, val_wt_loss, Metrics_list= val_one_epoch(model, Metrics, val_loader, loss_function, epoch, device)
            
            # è®¡ç®—å¹³å‡loss
            val_mean_loss = val_running_loss / len(val_loader)
            mean_val_et_loss = val_et_loss / len(val_loader)
            mean_val_tc_loss = val_tc_loss / len(val_loader)
            mean_val_wt_loss = val_wt_loss / len(val_loader)
            Metrics_list = Metrics_list / len(val_loader)
            
            # è®°å½•éªŒè¯ç»“æœ
            val_scores = {}
            val_scores['epoch'] = epoch
            val_scores['Dice_scores'] = Metrics_list[0] 
            val_scores['Jaccard_scores'] = Metrics_list[1]
            val_scores['Accuracy_scores'] = Metrics_list[2]
            val_scores['Precision_scores'] = Metrics_list[3]
            val_scores['Recall_scores'] = Metrics_list[4]
            val_scores['F1_scores'] = Metrics_list[5]
            val_scores['F2_scores'] = Metrics_list[6]
            # val_metrics.append(val_scores)
            
            """-------------------------------------- TensorBoard è®°å½•éªŒè¯ç»“æœ --------------------------------------------------"""
            writer.add_scalars(f'{loss_func_name}/val', 
                               {'Mean':val_mean_loss, 'ET': mean_val_et_loss, 'TC': mean_val_tc_loss, 'WT': mean_val_wt_loss}, 
                               epoch)
            # è®°å½•å¯¹æ¯”ç»“æœ
            writer.add_scalars(f'{loss_func_name}/Mean', 
                               {'Train':train_mean_loss, 'Val':val_mean_loss}, 
                               epoch)
            writer.add_scalars(f'{loss_func_name}/ET',
                               {'Train':mean_train_et_loss, 'Val':mean_val_et_loss}, 
                               epoch)
            writer.add_scalars(f'{loss_func_name}/TC',
                               {'Train':mean_train_tc_loss, 'Val':mean_val_tc_loss}, 
                               epoch)
            writer.add_scalars(f'{loss_func_name}/WT',
                               {'Train':mean_train_wt_loss, 'Val':mean_val_wt_loss}, 
                               epoch)

            if epoch == start_epoch+1:
                # åå°å¯åŠ¨tensorboardsé¢æ¿
                start_tensorboard(tb_dir, port=TB_PORT)

            if tb: 
                writer.add_scalars('metrics/Dice_coeff',
                                    {'Mean':val_scores['Dice_scores'][0], 'ET': val_scores['Dice_scores'][1], 'TC': val_scores['Dice_scores'][2], 'WT': val_scores['Dice_scores'][3]},
                                    epoch)

                writer.add_scalars('metrics/Jaccard_index', {'Mean':val_scores['Jaccard_scores'][0], 'ET': val_scores['Jaccard_scores'][1], 'TC': val_scores['Jaccard_scores'][2], 'WT': val_scores['Jaccard_scores'][3]},
                                    epoch)   

                writer.add_scalars('metrics/Accuracy',
                                    {'Mean':val_scores['Accuracy_scores'][0], 'ET': val_scores['Accuracy_scores'][1], 'TC': val_scores['Accuracy_scores'][2], 'WT': val_scores['Accuracy_scores'][3]},
                                    epoch)
                
                writer.add_scalars('metrics/Precision', 
                                    {'ET': val_scores['Precision_scores'][1], 'TC': val_scores['Precision_scores'][2], 'WT': val_scores['Precision_scores'][3]},
                                    epoch)
                
                writer.add_scalars('metrics/Recall', 
                                    {'Mean':val_scores['Recall_scores'][0], 'ET': val_scores['Recall_scores'][1], 'TC': val_scores['Recall_scores'][2], 'WT': val_scores['Recall_scores'][3]},
                                    epoch)
                writer.add_scalars('metrics/F1', 
                                    {'Mean':val_scores['F1_scores'][0], 'ET': val_scores['F1_scores'][1], 'TC': val_scores['F1_scores'][2], 'WT': val_scores['F1_scores'][3]},
                                    epoch) 
                writer.add_scalars('metrics/F2', 
                                    {'Mean':val_scores['F2_scores'][0], 'ET': val_scores['F2_scores'][1], 'TC': val_scores['F2_scores'][2], 'WT': val_scores['F2_scores'][3]},
                                    epoch)                               
            
            end_time = time.time()
            val_cost_time = end_time - start_time
            
            """-------------------------------------- æ‰“å°æŒ‡æ ‡ --------------------------------------------------"""
            metric_table_header = ["Metric_Name", "MEAN", "ET", "TC", "WT"]
            metric_table_left = ["Dice", "Jaccard", "Accuracy", "Precision", "Recall", "F1", "F2"]


            val_info_str =  f"=== [Epoch {epoch}/{end_epoch}] ===\n"\
                            f"- Model:    {model_name}\n"\
                            f"- Optimizer:{optimizer_name}\n"\
                            f"- Scheduler:{scheduler_name}\n"\
                            f"- LossFunc: {loss_func_name}\n"\
                            f"- Lr:       {current_lr:.6f}\n"\
                            f"- val_cost_time:{val_cost_time:.4f}s â±ï¸\n"

            # ä¼˜åŒ–ç‚¹ï¼šç›´æ¥é€šè¿‡æ˜ å°„è·å–æŒ‡æ ‡åç§°ï¼Œé¿å…é‡å¤å­—ç¬¦ä¸²æ ¼å¼åŒ–
            def format_value(value, decimals=4):
                # è¿”å›ä¸€ä¸ªæ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œä¿ç•™æŒ‡å®šçš„å°æ•°ä½æ•°
                return f"{value:.{decimals}f}"
            
            metric_scores_mapping = {metric: val_scores[f"{metric}_scores"] for metric in metric_table_left}
            metric_table = [[metric,
                            format_value(metric_scores_mapping[metric][0]),
                            format_value(metric_scores_mapping[metric][1]),
                            format_value(metric_scores_mapping[metric][2]),
                            format_value(metric_scores_mapping[metric][3])] for metric in metric_table_left]
            loss_str = f"Mean Loss: {val_mean_loss:.4f}, ET: {mean_val_et_loss:.4f}, TC: {mean_val_tc_loss:.4f}, WT: {mean_val_wt_loss:.4f}\n"
            table_str = tabulate(metric_table, headers=metric_table_header, tablefmt='grid')
            metrics_info = val_info_str + table_str + '\n' + loss_str  
            
            # å°†æŒ‡æ ‡è¡¨æ ¼å†™å…¥æ—¥å¿—æ–‡ä»¶
            custom_logger(metrics_info, logs_path)
            print(metrics_info)
            
            """------------------------------------- ä¿å­˜æƒé‡æ–‡ä»¶ --------------------------------------------"""
            best_dice = val_scores['Dice_scores'][0]
            # last_ckpt_path = os.path.join(ckpt_dir, f'{model_name}_braTS21_{loss_func_name}_{get_current_date() + ' ' + get_current_time()}_{epoch}_{val_mean_loss:.4f}_{best_dice:.4f}.pth')

            if val_mean_loss < best_val_loss:
                early_stopping_counter = 0
                best_val_loss = val_mean_loss
                best_epoch = epoch
                with open(os.path.join(os.path.dirname(logs_path), "current_log.txt"), 'a') as f:
                    f.write(f"=== Best EPOCH {best_epoch} ===:\n"\
                            f"@ {get_current_date() + ' ' + get_current_time()}\n"\
                            f"current lr : {current_lr:.6f}\n"\
                            f"loss: Mean:{val_mean_loss:.4f}\t ET: {mean_val_et_loss:.4f}\t TC: {mean_val_tc_loss:.4f}\t WT: {mean_val_wt_loss:.4f}\n"
                            f"mean dice : {val_scores['Dice_scores'][0]:.4f}\t" \
                            f"ET : {val_scores['Dice_scores'][1]:.4f}\t"\
                            f"TC : {val_scores['Dice_scores'][2]:.4f}\t" \
                            f"WT : {val_scores['Dice_scores'][3]:.4f}\n\n")
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_counter += 1
                best_ckpt_path = os.path.join(ckpt_dir, f'best@e{best_epoch}_{model_name}__{loss_func_name.lower()}{best_val_loss:.4f}_dice{best_dice:.4f}_{date_time_str}_{save_counter}.pth')
                if save_counter > save_max:
                    removed_ckpt = [ckpt for ckpt in os.listdir(ckpt_dir) if (ckpt.endswith('.pth') and (int(ckpt.split('.')[-2].split('_')[-1]) == int(save_counter - save_max)))] # è·å–è¦åˆ é™¤çš„æ–‡ä»¶å
                    os.remove(os.path.join(ckpt_dir, removed_ckpt[0]))
                    print(f"ğŸ—‘ï¸ Due to reach the max save amount, Removed {removed_ckpt[0]}")
                    save_checkpoint(model, optimizer, scaler, best_epoch, best_val_loss, best_ckpt_path)
                else:
                    save_checkpoint(model, optimizer, scaler, best_epoch, best_val_loss, best_ckpt_path)
            else:
                # æ—©åœç­–ç•¥ï¼Œå¦‚æœè¿ç»­patienceä¸ªepochæ²¡æœ‰æ”¹è¿›ï¼Œåˆ™åœæ­¢è®­ç»ƒ
                if early_stopping_counter == 0 :
                    continue
                else:
                    early_stopping_counter += 1
                    if early_stopping_counter >= early_stopping_patience:
                        print(f"ğŸƒ Early stopping at epoch {epoch} due to no improvement in validation loss.")
                        break
            
    print(f"ğŸ˜ƒğŸ˜ƒğŸ˜ƒTrain finished. Best val loss: ğŸ‘‰{best_val_loss:.4f} at epoch {best_epoch}")
    # è®­ç»ƒå®Œæˆåå…³é—­ SummaryWriter
    writer.close() 
    # å°†æœ€åä¸€ä¸ªä¿å­˜çš„æƒé‡æ–‡ä»¶é‡å‘½åä¸º final_model.pth

    # è·å–æ£€æŸ¥ç‚¹ç›®å½•ä¸­æ‰€æœ‰.pthæ–‡ä»¶
    ckpt_files = [f for f in os.listdir(ckpt_dir) if f.endswith('.pth')]

    if ckpt_files:
        # æŒ‰æœ€åä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        latest_ckpt = max(ckpt_files, key=lambda f: os.path.getmtime(os.path.join(ckpt_dir, f)))
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        latest_ckpt_path = os.path.join(ckpt_dir, latest_ckpt)
        final_model_path = os.path.join(ckpt_dir, f'{model_name}_final_model.pth')
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy(latest_ckpt_path, final_model_path)
        print(f"âœ… æœ€åä¸€ä¸ªæƒé‡æ–‡ä»¶å·²å¤åˆ¶ä¸º {final_model_path}")
    else:
        print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æƒé‡æ–‡ä»¶åœ¨ {ckpt_dir}")
    
    output_path = os.path.join(output_path, model_name, get_current_date()+'_'+get_current_time())
    # è‡ªåŠ¨æ¨ç†
    inference(
        test_df=test_df,
        test_loader=test_loader, 
        output_path=output_path, 
        model=model,
        optimizer=optimizer, 
        scaler=scaler,
        ckpt_path=final_model_path,
        window_size=(128, 128, 128), 
        stride_ratio=0.5, 
        save_flag=True,
        device=DEVICE
        )
    print(f"ğŸ‰ğŸ‰ğŸ‰æ¨ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ {output_path}")
    
    

    